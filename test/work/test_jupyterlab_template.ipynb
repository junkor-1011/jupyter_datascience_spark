{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n",
    "=======\n",
    "\n",
    "**\\* Please copy & rename me from `test_jupyterlab_template.ipynb`**\n",
    "\n",
    "- check primary tools if they work well.\n",
    "\n",
    "\n",
    "ToDo:\n",
    "* add test\n",
    "    - `cython`, `numba`, `numexpr`, e.t.c.\n",
    "* add function and modify bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## pandas & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# iris.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jupyterlab widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "iris.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot_bokeh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.from_pandas(iris, chunksize=4)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Appendix) matplotlib日本語化\n",
    "\n",
    "Ref: https://qiita.com/maroKanatani/items/3b080c639395bba7795a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dummy_data = np.random.randn(300)\n",
    "plt.hist(dummy_data)\n",
    "# ひらがな、カタカナ、漢字、全角文字の全てが表示されることを確認（IPAexゴシックなど）\n",
    "plt.xlabel(\"横方向の軸（X軸）\")\n",
    "plt.ylabel(\"縦方向の軸（X軸）\")\n",
    "plt.title(\"ヒストグラムのタイトル\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris: already created\n",
    "df = spark.createDataFrame(iris)\n",
    "df.show()\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('species').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('species').avg().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hive-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database test\")\n",
    "df.write.saveAsTable(\"test.iris\", format=\"orc\", compression=\"zlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.table(\"test.iris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphFrames\n",
    "\n",
    "Ref: https://graphframes.github.io/graphframes/docs/_site/user-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark._sc) # or, sqlContext = SQLContext(spark)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# spark = SparkSession.builder.appName('sample').getOrCreate()\n",
    "# sqlContext = SQLContext(spark)\n",
    "\n",
    "# Vertex DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 36),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Edge DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "from graphframes.examples import Graphs\n",
    "g = Graphs(spark).friends()\n",
    "\n",
    "g.vertices.show()\n",
    "g.edges.show()\n",
    "\n",
    "vertexInDegrees = g.inDegrees\n",
    "vertexInDegrees.show()\n",
    "\n",
    "g.vertices.groupBy().min(\"age\").show()\n",
    "\n",
    "numFollows = g.edges.filter(\"relationship = 'follow'\").count()\n",
    "print(numFollows)\n",
    "\n",
    "# motif\n",
    "motifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\n",
    "motifs.filter(\"b.age > 30\").show()\n",
    "\n",
    "chain4 = g.find(\"(a)-[ab]->(b); (b)-[bc]->(c); (c)-[cd]->(d)\")\n",
    "\n",
    "sumFriends =\\\n",
    "  lambda cnt,relationship: when(relationship == \"friend\", cnt+1).otherwise(cnt)\n",
    "condition =\\\n",
    "  reduce(lambda cnt,e: sumFriends(cnt, col(e).relationship), [\"ab\", \"bc\", \"cd\"], lit(0))\n",
    "chainWith2Friends2 = chain4.where(condition >= 2)\n",
    "chainWith2Friends2.show()\n",
    "\n",
    "# subgraph\n",
    "g1 = g.filterVertices(\"age > 30\").filterEdges(\"relationship = 'friend'\").dropIsolatedVertices()\n",
    "g1.vertices.show()\n",
    "g1.edges.show()\n",
    "\n",
    "paths = g.find(\"(a)-[e]->(b)\")\\\n",
    "  .filter(\"e.relationship = 'follow'\")\\\n",
    "  .filter(\"a.age < b.age\")\n",
    "e2 = paths.select(\"e.src\", \"e.dst\", \"e.relationship\")\n",
    "g2 = GraphFrame(g.vertices, e2)\n",
    "g2.vertices.show()\n",
    "g2.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it may take long time to execute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page rank\n",
    "\n",
    "# from graphframes.examples import Graphs\n",
    "# g = Graphs(spark).friends()  # Get example graph\n",
    "\n",
    "# # Run PageRank until convergence to tolerance \"tol\".\n",
    "# results = g.pageRank(resetProbability=0.15, tol=0.01)\n",
    "# # Display resulting pageranks and final edge weights\n",
    "# # Note that the displayed pagerank may be truncated, e.g., missing the E notation.\n",
    "# # In Spark 1.5+, you can use show(truncate=False) to avoid truncation.\n",
    "# results.vertices.select(\"id\", \"pagerank\").show()\n",
    "# results.edges.select(\"src\", \"dst\", \"weight\").show()\n",
    "\n",
    "# # Run PageRank for a fixed number of iterations.\n",
    "# results2 = g.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "# # Run PageRank personalized for vertex \"a\"\n",
    "# results3 = g.pageRank(resetProbability=0.15, maxIter=10, sourceId=\"a\")\n",
    "\n",
    "# # Run PageRank personalized for vertex [\"a\", \"b\", \"c\", \"d\"] in parallel\n",
    "# results4 = g.parallelPersonalizedPageRank(resetProbability=0.15, sourceIds=[\"a\", \"b\", \"c\", \"d\"], maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "belows may not work...\n",
    "\n",
    "(cause `Analysis Execption` in `Scala`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest path\n",
    "# sqlContext = SQLContext(spark)\n",
    "# from graphframes.examples import Graphs\n",
    "# g = Graphs(sqlContext).friends()  # Get example graph\n",
    "\n",
    "# results = g.shortestPaths(landmarks=[\"a\", \"d\"])\n",
    "# results.select(\"id\", \"distances\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangle count\n",
    "# sqlContext = SQLContext(spark)\n",
    "# from graphframes.examples import Graphs\n",
    "# g = Graphs(sqlContext).friends()  # Get example graph\n",
    "\n",
    "# results = g.triangleCount()\n",
    "# results.select(\"id\", \"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "- investigate the reasons of above `Analysis Exception`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "folium.Map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### osmnx\n",
    "\n",
    "Ref: https://github.com/gboeing/osmnx-examples/blob/master/notebooks/00-osmnx-features-demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a graph for some city\n",
    "G = ox.graph_from_place('Piedmont, California, USA', network_type='drive')\n",
    "fig, ax = ox.plot_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'amenity' : True,\n",
    "        'landuse' : ['retail', 'commercial'],\n",
    "        'highway' : 'bus_stop'}\n",
    "gdf = ox.pois_from_place('Piedmont, California, USA', tags)\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf['highway']=='bus_stop'].dropna(axis=1, how='any').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoViews\n",
    "\n",
    "Ref: https://geoviews.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import xarray as xr\n",
    "from cartopy import crs\n",
    "\n",
    "gv.extension('bokeh', 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf.ocean + gf.land + gf.ocean * gf.land * gf.coastline * gf.borders).opts(\n",
    "    'Feature', projection=crs.Geostationary(), global_extent=True, height=325).cols(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gv.Polygons(gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')), vdims=['pop_est', ('name', 'Country')]).opts(\n",
    "    tools=['hover'], width=600, projection=crs.Robinson()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geoplot\n",
    "\n",
    "Ref: https://github.com/ResidentMario/geoplot/blob/master/examples/plot_boston_airbnb_kde.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import geoplot.crs as gcrs\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplleaflet\n",
    "\n",
    "boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))\n",
    "\n",
    "ax = gplt.kdeplot(\n",
    "    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),\n",
    "    shade=True\n",
    ")\n",
    "gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)\n",
    "gplt.webmap(boston_airbnb_listings, ax=ax)\n",
    "plt.title('Boston AirBnB Locations, 2016', fontsize=18)\n",
    "\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "Ref: https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# positive integer target correlated with X[:, 5] with many zeros:\n",
    "y = rng.poisson(lam=np.exp(X[:, 5]) / 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "glm = PoissonRegressor()\n",
    "gbdt = HistGradientBoostingRegressor(loss='poisson', learning_rate=.01)\n",
    "glm.fit(X_train, y_train)\n",
    "gbdt.fit(X_train, y_train)\n",
    "print(glm.score(X_test, y_test))\n",
    "print(gbdt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "set_config(display='diagram')\n",
    "\n",
    "num_proc = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "\n",
    "cat_proc = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((num_proc, ('feat1', 'feat3')),\n",
    "                                       (cat_proc, ('feat0', 'feat2')))\n",
    "\n",
    "clf = make_pipeline(preprocessor, LogisticRegression())\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import completeness_score\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_blobs(random_state=rng)\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "X_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\n",
    "kmeans = KMeans(algorithm='elkan').fit(X_train)\n",
    "print(completeness_score(kmeans.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples = 500\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, 2)\n",
    "noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n",
    "y = (5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise)\n",
    "\n",
    "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\n",
    "gbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n",
    "\n",
    "disp = plot_partial_dependence(\n",
    "    gbdt_no_cst, X, features=[0], feature_names=['feature 0'],\n",
    "    line_kw={'linewidth': 4, 'label': 'unconstrained'})\n",
    "plot_partial_dependence(gbdt_cst, X, features=[0],\n",
    "    line_kw={'linewidth': 4, 'label': 'constrained'}, ax=disp.axes_)\n",
    "disp.axes_[0, 0].plot(X[:, 0], y, 'o', alpha=.5, zorder=-1, label='samples')\n",
    "disp.axes_[0, 0].set_ylim(-3, 3); disp.axes_[0, 0].set_xlim(-1, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_regression(n_samples, n_features, random_state=rng)\n",
    "sample_weight = rng.rand(n_samples)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "    X, y, sample_weight, random_state=rng)\n",
    "reg = Lasso()\n",
    "reg.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(reg.score(X_test, y_test, sw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `pydotplus` -> png, PDF\n",
    "\n",
    "Ref: https://mk-55.hatenablog.com/entry/2019/02/09/032646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "from graphviz import Digraph\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2)  # limit depth of tree\n",
    "iris = load_iris()\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "dot_data = tree.export_graphviz(\n",
    "    clf,\n",
    "    out_file=None,\n",
    "    feature_names=['がく片の長さ','がく片の幅','花弁の長さ','花弁の幅'],\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    proportion=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "#Setting font for Node \n",
    "graph.set_fontname('MS UI Gothic')\n",
    "\n",
    "#Setting font for Node \n",
    "for node in graph.get_nodes():\n",
    "    node.set_fontname('MS UI Gothic')\n",
    "\n",
    "#Setting font for Edges \n",
    "for e in graph.get_edges():\n",
    "    e.set_fontname('MS UI Gothic')\n",
    "\n",
    "graph.write_pdf(\"pydotplus_test.pdf\")\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Appendix)pygraphviz\n",
    "\n",
    "Ref: http://april.fool.jp/blogs/2013/12/17/graphviz%E3%81%A7%E3%82%AB%E3%83%83%E3%82%AF%E3%81%84%E3%81%84%E3%82%B0%E3%83%A9%E3%83%95%E3%82%92%E6%8F%8F%E3%81%93%E3%81%86/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pygraphviz as pgv\n",
    "G = pgv.AGraph()\n",
    "\n",
    "G.add_node('a')\n",
    "G.add_node('b')\n",
    "\n",
    "G.add_edge('c','d')\n",
    "G.add_edge('e','f')\n",
    "\n",
    "G.layout()\n",
    "G.draw('sample01.png')\n",
    "G.draw(\"sample01.pdf\")\n",
    "G.write(\"sample01.dot\")\n",
    "Image(\"sample01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "* ~`pydot` -> PDF~\n",
    "* `sklearn.external.six`, `StringIO`\n",
    "* ~`pygraphviz`~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://blog.amedama.jp/entry/2019/01/29/235642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"XGBoost で early_stopping_rounds を使って学習ラウンド数を最適化するサンプルコード\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = datasets.load_breast_cancer()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    evals_result = {}\n",
    "    bst = xgb.train(xgb_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=1000,\n",
    "                    # 一定ラウンド回しても改善が見込めない場合は学習を打ち切る\n",
    "                    early_stopping_rounds=10,\n",
    "                    evals=evals,\n",
    "                    evals_result=evals_result,\n",
    "                    )\n",
    "\n",
    "    y_pred_proba = bst.predict(dtest)\n",
    "    y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    train_metric = evals_result['train']['logloss']\n",
    "    plt.plot(train_metric, label='train logloss')\n",
    "    eval_metric = evals_result['eval']['logloss']\n",
    "    plt.plot(eval_metric, label='eval logloss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('rounds')\n",
    "    plt.ylabel('logloss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://blog.amedama.jp/entry/2018/05/01/081842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"LightGBM を使った多値分類のサンプルコード (CV)\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Iris データセットを読み込む\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X, y)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 多値分類問題\n",
    "        'objective': 'multiclass',\n",
    "        # クラス数は 3\n",
    "        'num_class': 3,\n",
    "    }\n",
    "\n",
    "    # 上記のパラメータでモデルを学習〜交差検証までする\n",
    "    cv_results = lgb.cv(lgbm_params, lgb_train, nfold=10)\n",
    "    cv_logloss = cv_results['multi_logloss-mean']\n",
    "    round_n = np.arange(len(cv_logloss))\n",
    "\n",
    "    plt.xlabel('round')\n",
    "    plt.ylabel('logloss')\n",
    "    plt.plot(round_n, cv_logloss)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost\n",
    "\n",
    "Ref: https://catboost.ai/docs/concepts/python-usages-examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "# Initialize data\n",
    "\n",
    "train_data = [[1, 4, 5, 6],\n",
    "              [4, 5, 6, 7],\n",
    "              [30, 40, 50, 60]]\n",
    "\n",
    "eval_data = [[2, 4, 6, 8],\n",
    "             [1, 4, 50, 60]]\n",
    "\n",
    "train_labels = [10, 20, 30]\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=2,\n",
    "                          learning_rate=1,\n",
    "                          depth=2)\n",
    "# Fit model\n",
    "model.fit(train_data, train_labels)\n",
    "# Get predictions\n",
    "preds = model.predict(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, min_x, max_x):\n",
    "        # Hold this implementation specific arguments as the fields of the class.\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        # Calculate an objective value by using the extra arguments.\n",
    "        x = trial.suggest_uniform('x', self.min_x, self.max_x)\n",
    "        return (x - 2) ** 2\n",
    "\n",
    "# Execute an optimization by using an `Objective` instance.\n",
    "study = optuna.create_study()\n",
    "study.optimize(Objective(-100, 100), n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### somoclu\n",
    "\n",
    "Ref: https://somoclu.readthedocs.io/en/stable/example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import somoclu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.random.rand(50, 3)/5\n",
    "c2 = (0.6, 0.1, 0.05) + np.random.rand(50, 3)/5\n",
    "c3 = (0.4, 0.1, 0.7) + np.random.rand(50, 3)/5\n",
    "data = np.float32(np.concatenate((c1, c2, c3)))\n",
    "colors = [\"red\"] * 50\n",
    "colors.extend([\"green\"] * 50)\n",
    "colors.extend([\"blue\"] * 50)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=colors)\n",
    "labels = range(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_columns = 100, 160\n",
    "som = somoclu.Somoclu(n_columns, n_rows, compactsupport=False)\n",
    "%time som.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_component_planes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bhtsne\n",
    "\n",
    "ToDo:\n",
    "* prepare example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### umap\n",
    "\n",
    "Ref: https://qiita.com/cheerfularge/items/27a55ebde4a671880666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    digits = load_digits()\n",
    "    digits.target = [float(digits.target[i]) for i in range(len(digits.target))]\n",
    "\n",
    "    # UMAP\n",
    "    start_time = time.time()\n",
    "    embedding = umap.UMAP().fit_transform(digits.data)\n",
    "    interval = time.time() - start_time\n",
    "    plt.scatter(embedding[:,0],embedding[:,1],c=digits.target,cmap=cm.tab10)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('umap.png')\n",
    "\n",
    "    # t-SNE\n",
    "    plt.clf()\n",
    "    start_time2 = time.time()\n",
    "    tsne_model = TSNE(n_components=2)\n",
    "    tsne = tsne_model.fit_transform(digits.data)\n",
    "    interval2 = time.time() - start_time2\n",
    "    plt.scatter(tsne[:,0],tsne[:,1],c=digits.target,cmap=cm.tab10)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('tsne.png')\n",
    "\n",
    "    print('umap : {}s'.format(interval))\n",
    "    print('tsne : {}s'.format(interval2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow2\n",
    "\n",
    "https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ja\n",
    "\n",
    "WARNING:\n",
    "* 原因は不明だが、上までの処理を行ってからtensorflowを読み込むと`tf.keras`を読み込めないなど意図しない挙動が起きている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env export -n base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda env export -n base | tee conda_packages_freeze.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
