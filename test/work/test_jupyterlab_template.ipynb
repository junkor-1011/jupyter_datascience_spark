{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n",
    "=======\n",
    "\n",
    "**\\* Please copy & rename me from `test_jupyterlab_template.ipynb`**\n",
    "\n",
    "- check primary tools if they work well.\n",
    "\n",
    "\n",
    "ToDo:\n",
    "* add test\n",
    "    - `cython`, `numba`, `numexpr`, e.t.c.\n",
    "* add function and modify bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## pandas & plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# iris.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jupyterlab widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "iris.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_bokeh\n",
    "pandas_bokeh.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot_bokeh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.from_pandas(iris, chunksize=4)\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris: already created\n",
    "df = spark.createDataFrame(iris)\n",
    "df.show()\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('species').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('species').avg().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hive-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database test\")\n",
    "df.write.saveAsTable(\"test.iris\", format=\"orc\", compression=\"zlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.table(\"test.iris\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession, SQLContext\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# spark = SparkSession.builder.appName('sample').getOrCreate()\n",
    "# sqlContext = SQLContext(spark)\n",
    "\n",
    "v = spark.createDataFrame([\n",
    "    (\"a\", \"Alice\", 34),\n",
    "    (\"b\", \"Bob\", 36),\n",
    "    (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "e = spark.createDataFrame([\n",
    "    (\"a\", \"b\", \"friend\"),\n",
    "    (\"b\", \"c\", \"follow\"),\n",
    "    (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "g = GraphFrame(v, e)\n",
    "g.inDegrees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "- enable `GraphFrames`\n",
    "    - need to setup config (designate jar file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "folium.Map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### osmnx\n",
    "\n",
    "from https://github.com/gboeing/osmnx-examples/blob/master/notebooks/00-osmnx-features-demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "ox.config(use_cache=True, log_console=True)\n",
    "ox.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a graph for some city\n",
    "G = ox.graph_from_place('Piedmont, California, USA', network_type='drive')\n",
    "fig, ax = ox.plot_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'amenity' : True,\n",
    "        'landuse' : ['retail', 'commercial'],\n",
    "        'highway' : 'bus_stop'}\n",
    "gdf = ox.pois_from_place('Piedmont, California, USA', tags)\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf['highway']=='bus_stop'].dropna(axis=1, how='any').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoViews\n",
    "\n",
    "from https://geoviews.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import xarray as xr\n",
    "from cartopy import crs\n",
    "\n",
    "gv.extension('bokeh', 'matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gf.ocean + gf.land + gf.ocean * gf.land * gf.coastline * gf.borders).opts(\n",
    "    'Feature', projection=crs.Geostationary(), global_extent=True, height=325).cols(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "gv.Polygons(gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')), vdims=['pop_est', ('name', 'Country')]).opts(\n",
    "    tools=['hover'], width=600, projection=crs.Robinson()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geoplot\n",
    "\n",
    "from https://github.com/ResidentMario/geoplot/blob/master/examples/plot_boston_airbnb_kde.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "import geoplot.crs as gcrs\n",
    "import matplotlib.pyplot as plt\n",
    "# import mplleaflet\n",
    "\n",
    "boston_airbnb_listings = gpd.read_file(gplt.datasets.get_path('boston_airbnb_listings'))\n",
    "\n",
    "ax = gplt.kdeplot(\n",
    "    boston_airbnb_listings, cmap='viridis', projection=gcrs.WebMercator(), figsize=(12, 12),\n",
    "    shade=True\n",
    ")\n",
    "gplt.pointplot(boston_airbnb_listings, s=1, color='black', ax=ax)\n",
    "gplt.webmap(boston_airbnb_listings, ax=ax)\n",
    "plt.title('Boston AirBnB Locations, 2016', fontsize=18)\n",
    "\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "from https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_0_23_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-0-23-0-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# positive integer target correlated with X[:, 5] with many zeros:\n",
    "y = rng.poisson(lam=np.exp(X[:, 5]) / 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "glm = PoissonRegressor()\n",
    "gbdt = HistGradientBoostingRegressor(loss='poisson', learning_rate=.01)\n",
    "glm.fit(X_train, y_train)\n",
    "gbdt.fit(X_train, y_train)\n",
    "print(glm.score(X_test, y_test))\n",
    "print(gbdt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "set_config(display='diagram')\n",
    "\n",
    "num_proc = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "\n",
    "cat_proc = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((num_proc, ('feat1', 'feat3')),\n",
    "                                       (cat_proc, ('feat0', 'feat2')))\n",
    "\n",
    "clf = make_pipeline(preprocessor, LogisticRegression())\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import completeness_score\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_blobs(random_state=rng)\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "X_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\n",
    "kmeans = KMeans(algorithm='elkan').fit(X_train)\n",
    "print(completeness_score(kmeans.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples = 500\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, 2)\n",
    "noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n",
    "y = (5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise)\n",
    "\n",
    "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\n",
    "gbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n",
    "\n",
    "disp = plot_partial_dependence(\n",
    "    gbdt_no_cst, X, features=[0], feature_names=['feature 0'],\n",
    "    line_kw={'linewidth': 4, 'label': 'unconstrained'})\n",
    "plot_partial_dependence(gbdt_cst, X, features=[0],\n",
    "    line_kw={'linewidth': 4, 'label': 'constrained'}, ax=disp.axes_)\n",
    "disp.axes_[0, 0].plot(X[:, 0], y, 'o', alpha=.5, zorder=-1, label='samples')\n",
    "disp.axes_[0, 0].set_ylim(-3, 3); disp.axes_[0, 0].set_xlim(-1, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_regression(n_samples, n_features, random_state=rng)\n",
    "sample_weight = rng.rand(n_samples)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "    X, y, sample_weight, random_state=rng)\n",
    "reg = Lasso()\n",
    "reg.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(reg.score(X_test, y_test, sw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "* `pydot` -> PDF\n",
    "    - `sklearn.external.six`, `StringIO`\n",
    "* `pygraphviz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://blog.amedama.jp/entry/2019/01/29/235642"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"XGBoost で early_stopping_rounds を使って学習ラウンド数を最適化するサンプルコード\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset = datasets.load_breast_cancer()\n",
    "    X, y = dataset.data, dataset.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.3,\n",
    "                                                        shuffle=True,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    evals_result = {}\n",
    "    bst = xgb.train(xgb_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=1000,\n",
    "                    # 一定ラウンド回しても改善が見込めない場合は学習を打ち切る\n",
    "                    early_stopping_rounds=10,\n",
    "                    evals=evals,\n",
    "                    evals_result=evals_result,\n",
    "                    )\n",
    "\n",
    "    y_pred_proba = bst.predict(dtest)\n",
    "    y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    train_metric = evals_result['train']['logloss']\n",
    "    plt.plot(train_metric, label='train logloss')\n",
    "    eval_metric = evals_result['eval']['logloss']\n",
    "    plt.plot(eval_metric, label='eval logloss')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('rounds')\n",
    "    plt.ylabel('logloss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://blog.amedama.jp/entry/2018/05/01/081842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"LightGBM を使った多値分類のサンプルコード (CV)\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Iris データセットを読み込む\n",
    "    iris = datasets.load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # データセットを生成する\n",
    "    lgb_train = lgb.Dataset(X, y)\n",
    "\n",
    "    # LightGBM のハイパーパラメータ\n",
    "    lgbm_params = {\n",
    "        # 多値分類問題\n",
    "        'objective': 'multiclass',\n",
    "        # クラス数は 3\n",
    "        'num_class': 3,\n",
    "    }\n",
    "\n",
    "    # 上記のパラメータでモデルを学習〜交差検証までする\n",
    "    cv_results = lgb.cv(lgbm_params, lgb_train, nfold=10)\n",
    "    cv_logloss = cv_results['multi_logloss-mean']\n",
    "    round_n = np.arange(len(cv_logloss))\n",
    "\n",
    "    plt.xlabel('round')\n",
    "    plt.ylabel('logloss')\n",
    "    plt.plot(round_n, cv_logloss)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "# Initialize data\n",
    "\n",
    "train_data = [[1, 4, 5, 6],\n",
    "              [4, 5, 6, 7],\n",
    "              [30, 40, 50, 60]]\n",
    "\n",
    "eval_data = [[2, 4, 6, 8],\n",
    "             [1, 4, 50, 60]]\n",
    "\n",
    "train_labels = [10, 20, 30]\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=2,\n",
    "                          learning_rate=1,\n",
    "                          depth=2)\n",
    "# Fit model\n",
    "model.fit(train_data, train_labels)\n",
    "# Get predictions\n",
    "preds = model.predict(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, min_x, max_x):\n",
    "        # Hold this implementation specific arguments as the fields of the class.\n",
    "        self.min_x = min_x\n",
    "        self.max_x = max_x\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        # Calculate an objective value by using the extra arguments.\n",
    "        x = trial.suggest_uniform('x', self.min_x, self.max_x)\n",
    "        return (x - 2) ** 2\n",
    "\n",
    "# Execute an optimization by using an `Objective` instance.\n",
    "study = optuna.create_study()\n",
    "study.optimize(Objective(-100, 100), n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### somoclu\n",
    "\n",
    "from https://somoclu.readthedocs.io/en/stable/example.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import somoclu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.random.rand(50, 3)/5\n",
    "c2 = (0.6, 0.1, 0.05) + np.random.rand(50, 3)/5\n",
    "c3 = (0.4, 0.1, 0.7) + np.random.rand(50, 3)/5\n",
    "data = np.float32(np.concatenate((c1, c2, c3)))\n",
    "colors = [\"red\"] * 50\n",
    "colors.extend([\"green\"] * 50)\n",
    "colors.extend([\"blue\"] * 50)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(data[:, 0], data[:, 1], data[:, 2], c=colors)\n",
    "labels = range(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_columns = 100, 160\n",
    "som = somoclu.Somoclu(n_columns, n_rows, compactsupport=False)\n",
    "%time som.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_component_planes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.view_umatrix(bestmatches=True, bestmatchcolors=colors, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bhtsne\n",
    "\n",
    "ToDo:\n",
    "* prepare example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### umap\n",
    "\n",
    "from https://qiita.com/cheerfularge/items/27a55ebde4a671880666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    digits = load_digits()\n",
    "    digits.target = [float(digits.target[i]) for i in range(len(digits.target))]\n",
    "\n",
    "    # UMAP\n",
    "    start_time = time.time()\n",
    "    embedding = umap.UMAP().fit_transform(digits.data)\n",
    "    interval = time.time() - start_time\n",
    "    plt.scatter(embedding[:,0],embedding[:,1],c=digits.target,cmap=cm.tab10)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('umap.png')\n",
    "\n",
    "    # t-SNE\n",
    "    plt.clf()\n",
    "    start_time2 = time.time()\n",
    "    tsne_model = TSNE(n_components=2)\n",
    "    tsne = tsne_model.fit_transform(digits.data)\n",
    "    interval2 = time.time() - start_time2\n",
    "    plt.scatter(tsne[:,0],tsne[:,1],c=digits.target,cmap=cm.tab10)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('tsne.png')\n",
    "\n",
    "    print('umap : {}s'.format(interval))\n",
    "    print('tsne : {}s'.format(interval2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow2\n",
    "\n",
    "https://www.tensorflow.org/tutorials/quickstart/beginner?hl=ja\n",
    "\n",
    "WARNING:\n",
    "* 原因は不明だが、上までの処理を行ってからtensorflowを読み込むと`tf.keras`を読み込めないなど意図しない挙動が起きている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env export -n base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda env export -n base > conda_package_freeze.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "- to clear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
